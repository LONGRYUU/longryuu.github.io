# 机器学习笔记
this is created for marking down some notes during study

## 算法

### 梯度下降
1、h(假设)：输出函数，即通过训练得到的一个函数结果。可以接收输入，获得输出。 

2、批量梯度下降（递归），随机梯度下降（循环）。

### 局部加权线性回归
1、对每个点进行拟合时，对其附近的点赋予高的权值，较远的点赋予低的权值。每一个点需要对所有数据拟合一次，开销较大。


## 概念

属性——特征，示例——样本——特征向量。

属性空间/样本空间/输入空间：属性张成的空间，在该空间内可以取得所有样本的值，换言之，每个样本对应该空间中的一个点，维度为属性的个数。

假设空间：所有假设的集合，即可能的模型或者学习结果的集合，（针对西瓜问题）可以表现为对属性取不同值进行搭配的集合。

版本空间：针对具体训练集的假设集合，这里的假设即根据训练集可以得到预期结果的模型，是假设空间的子集。

标记：关于示例的结果的信息（可以理解为样本处理的结果），被标记的示例成为样例，所有标记的集合称为标记空间或输出空间。

假设：从数据中会获得一个模型，模型对应了某种规律，这就是假设，规律本身称作真相或者真实。

分类：结果是离散的。 回归：结果是连续的。

泛化：模型适用于新样本的能力。

精度=1-错误率。 错误率=错误样本数/总样本数。

查准率=TP/(TP+FP),查全率=TP/(TP+FN)。

F1度量=2*P*R/（P+R），其拥有更为一般的形式，当参数β大于1，使查全率更重要；当其小于1，使查准率更重要。

P-R图与ROC图。

ROC相关：真正例率TPR=TP/(TP+FN)，假正例率FPR=FP/(TN+FP)

损失与AUC：设损失为l，则AUC=1-l,其中AUC为ROC曲线下面积，l即为ROC曲线上的面积

非均等代价：为不同错误赋予不同的代价（cost）。

代价敏感错误率：在错误率的基础上，给不同错误发生的次数乘以代价权值。

代价曲线：由多段直线构成，每一条直线对应ROC图上的一个点，曲线下的面积是期望的总体代价；线段由（0，FPR）到（1，FNR）。


## 模型评估

留出法：直接将数据集划分为两个互斥的集合。

交叉验证法：将数据集划分为k个互斥子集，每次选择k-1个作为训练集，余下的作为测试集，进行k次训练和测试，取平均结果

自助法：每次从含有m个样本的数据集中拷贝一个样本，重复m次，得到一个训练集将（依概率来看）包含约2/3的样本，剩余样本作为测试集。一半用于数据集较小时，数据充足时一般使用前两种办法。


## 线性模型

### 线性回归

求一组参数，向量w，和参数b，构成直线，使得所有样本点到直线的距离之和最短。
可以采用最小二乘法，对w和b分别求偏导，使得结果为0.

### 对数几率函数

对于处理二分类时，为了使得函数连续可微而构造的函数。

对数几率：ln[y/(1-y)],其中的y/(1-y)称为几率。

### 线性判别分析

在二分类问题上，将样例投影到一条直线，使得同类样例尽可能接近，异类样例尽可能远离。即要使得同类样例投影点的协方差尽可能小，而使得异类样例投影点的类中心距离尽可能大。

### 多分类问题的求解策略

拆解成若干个二分类问题。拆解的策略有一对一，一对其余，多对多。

一对一分类：将N个预期的分类结果两两配对得到(N-1)N/2个分类器，每次分别将样本提交给各个分类器得到分类结果，得到的(N-1)N/2个结果中，类别最多的即为所求分类。

一对其余：需要N个分类器，每个分类器的正类为N个类别中的一个（各不相同），其余类别作为反类；则当N个分类器中，仅有一个结果为正类，则取该正类作为最终分类结果，而若有多个预测为正类，则需要另外考虑置信度。

多对多：每次将若干的划分为正类，其余的划分为反类，不可随意选取。

纠错输出码：类别越多，编码越长，纠错能力越强。如果类别和编码长度一定，需要使得不同类别的编码差异越大，从而使得纠错能力越强

编码：对N个类别进行M次划分，每次划分将一部分划分为正类，一部分划分为反类，产生M个训练集以得到M个分类器。

解码：M个分类器分别对样本进行测试，所得预测标记组成一个编码，将该编码与各个类别各自的编码进行比较，差别最小的即为所求类别。

### 类别不平衡问题

分类任务中，训练样例数的差别很大，或者在进行多分类拆分时，得到的类别数量不平衡。

再缩放法：对于几率y/(1-y)，给它乘以比例（反例数/正例数），构成新的几率，当该值大于1则为正例。局限：训练集样本难以和真实样本毫无偏差。

欠采样法：去除一些示例使得正反例的数目接近。

过采样法：增加一些示例使得正反例数目接近。

阈值移动：将再缩放嵌入决策中。

## 决策树
